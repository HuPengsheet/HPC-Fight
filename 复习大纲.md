# 高性能计算求职面试全攻略



# c/c++

### c++内存布局

![image-20240611122949435](/home/hp/code/HPC-Fight/HPC-Fight/image/image-20240611122949435.png)

```
常量区：虚函数表、const修饰的全局变量、字符串常量
程序代码：函数、类函数
全局数据区：全局变量、静态变量
.bss:.主要作用是存放程序中未初始化的全局变量和静态变量。这些变量在编译时没有明确赋初值，因此在程序加载到内存时，系统会自动将.bss段的内存空间清零，以保证变量在使用前具备确定的初值。

```



### 构造函数

```c++
#include <iostream>  
#include <cstring>  
  
class String {  
private:  
    char* data;  
    size_t length;  
  
public:  
    // 默认构造函数，创建一个空字符串  
    String() : data(nullptr), length(0) {  
    }  
  
    // 构造函数，接收一个C风格的字符串并复制其内容  
    String(const char* str) {  
        length = strlen(str);  
        data = (char*)malloc(length + 1); // 分配内存，+1用于字符串结束符'\0'  
        if (data != nullptr) {  
            strcpy(data, str); // 复制字符串  
        }  
    }  
  
    // 拷贝构造函数  
    String(const String& other) {  
        length = other.length;  
        data = (char*)malloc(length + 1); // 分配内存  
        if (data != nullptr) {  
            strcpy(data, other.data); // 复制字符串  
        }  
    }  
  
    // 析构函数，释放之前分配的内存  
    ~String() {  
        free(data); // 释放内存  
    }  
  
    // 拷贝赋值运算符  
    String& operator=(const String& other) {  
        if (this != &other) { // 避免自赋值  
            free(data); // 释放原有内存  
            length = other.length;  
            data = (char*)malloc(length + 1); // 分配新内存  
            if (data != nullptr) {  
                strcpy(data, other.data); // 复制字符串  
            }  
        }  
        return *this;  
    }  
  
    // 获取字符串长度  
    size_t getLength() const {  
        return length;  
    }  
  
    // 打印字符串  
    void print() const {  
        if (data != nullptr) {  
            std::cout << data << std::endl;  
        } else {  
            std::cout << "Empty string" << std::endl;  
        }  
    }  
  
    // 其他成员函数可以在这里添加  
};  
  

```

#### C++所有构造函数

类对象被创建时，编译器为对象分配内存空间，并自动调用构造函数，由构造函数完成成员的初始化工作。

因此构造函数的的作用是初始化对象的成员函数。

**默认构造函数：**如果没有人为构造函数，则编译器会自动默认生成一个无参构造函数。

**一般构造函数：**包含各种参数，一个类可以有多个一般构造函数，前提是参数的个数和类型和传入参数的顺序都不相同，根据传入参数调用对应的构造函数。

**拷贝构造函数：**拷⻉构造函数的函数参数为对象本身的引用，用于根据⼀个已存在的对象复制出⼀个新的该类的对象，⼀般在函数中会将已存在的对象的数据成员的值⼀⼀复制到新创建的对象中。如果没有显示的写拷⻉构造函数，则系统会默认创建⼀个拷⻉构造函数，但当类中有指针成员时，最好不要使⽤编译器提供的默认的拷⻉构造函 数，最好⾃⼰定义并且在函数中执⾏深拷⻉。

**移动构造函数：**有时候我们会遇到这样一种情况，我们用对象a初始化对象b后对象a我们就不在使用了，但是对象a的空间还在呀（在析构之前），既然拷贝构造函数，实际上就是把a对象的内容复制一份到b中，那么为什么我们不能直接使用a的空间呢？这样就避免了新的空间的分配，大大降低了构造的成本。这就是移动构造函数设计的初衷。拷贝构造函数中，对于指针，我们一定要采用深层复制，而移动构造函数中，对于指针，我们采用浅层复制。

> 但是指针的浅层复制是非常危险的。浅层复制之所以危险，是因为两个指针共同指向一片内存空间，若第一个指针将其释放，另一个指针的指向就不合法了（pointer dangling）。所以我们只要避免第一个指针释放空间就可以了。避免的方法就是将第一个指针（比如a->value）置为NULL，这样在调用析构函数的时候，由于有判断是否为NULL的语句，所以析构a的时候并不会回收a->value指向的空间（同时也是b->value指向的空间）

**赋值构造函数：**=运算符的重载，类似拷贝构造函数，将=右边的类对象赋值给类对象左边的对象，不属于构造函数，=两边的对象必须都要被创建。

**类型转换构造函数：**有时候不想要隐式转换，用explict关键字修饰。一般来说带一个参数的构造函数，或者其他参数是默认的构造函数      



#### 什么情况下会调用拷贝构造函数？

1. 对象以值传递的方式进入函数体
2. 对象以值传递的方式从函数返回
3. 一个对象需要另外一个对象初始化



#### 为什么拷贝构造函数必须是引用？

**为了防止递归调用。**

如果不用引用，就会是值传递的方式，但是值传递会调用拷贝构造函数生成临时对象，从而又调用一次拷贝构造函数。就这样无穷的递归下去。

**如果是指针类型**

就变成了一个带参数的构造函数了。。。

比如`A(A* test)`



#### 构造函数析构函数是否能抛出异常

**构造函数可以抛出异常**

对象只有在构造函数执行完成之后才算构造妥当，c++只会析构已经完成的对象。因此如果构造函数中发生异常，控制权就需要转移出构造函数，执行异常处理函数。在这个过程中系统会认为对象没有构造成功，导致不会调用析构函数。在构造函数中抛出异常会导致当前函数执行流程终止，在构造函数流程前构造的成员对象会被释放，但是如果在构造函数中申请了内存操作，则会造成内存泄漏。另外，如果有继承关系，派生类中的构造函数抛出异常，那么基类的构造函数和析构函数可以照常执行的。

解决办法：用智能指针来管理内存就可以

**C++标准指明析构函数不能、也不应该抛出异常**

C++异常处理模型最大的特点和优势就是对C++中的面向对象提供了最强大的无缝支持。那么如果对象在运行期间出现了异常，C++异常处理模型有责任清除那些由于出现异常所导致的已经失效了的对象(也即对象超出了它原来的作用域)，并释放对象原来所分配的资源， 这就是调用这些对象的析构函数来完成释放资源的任务，所以从这个意义上说，析构函数已经变成了异常处理的一部分。

析构函数不能抛出异常原因有两个：

1. 如果析构函数抛出异常，则异常点之后的程序不会执行，如果析构函数在异常点之后执行了某些必要的动作比如释放某些资源，则这些动作不会执行，会造成诸如资源泄漏的问题。
2. 异常发生时，c++的异常处理机制在异常的传播过程中会进行栈展开（stack-unwinding）。在栈展开的过程中就会调用已经在栈构造好的对象的析构函数来释放资源，此时若其他析构函数本身也抛出异常，则前一个异常尚未处理，又有新的异常，会造成程序崩溃。

解决办法：把异常完全封装在析构函数内部，决不让异常抛出函数之外，代码如下：

```c++
DBConn::~DBconn()
{
    try
    {
	    db.close(); 
    }
    catch(...)
    {
        abort();
    }
}
//如果close抛出异常就结束程序，通常调用abort完成：
```



#### 创建派生类对象，构造函数的执行顺序是什么？析构函数的执行顺序？

首先要知道类的构造函数不能被继承，构造函数不能被继承是有道理的，因为即使继承了，它的名字和派生类的名字也不一样（构造函数名字和类名一样），不能成为派生类的构造函数，当然更不能成为普通的成员函数。在设计派生类时，对继承过来的成员变量的初始化工作也要由派生类的构造函数完成，但是大部分基类都有private属性的成员变量，它们在派生类中无法访问，更不能使用派生类的构造函数来初始化。这种矛盾在C++继承中是普遍存在的，解决这个问题的思路是：**在派生类的构造函数中调用基类的构造函数**。

对象创建时候执行顺序是：**静态代码 --> 非静态代码 --> 构造函数。**静态代码包括（静态方法，静态变量，静态代码块等），非静态代码即（成员方法，成员变量，成员代码块等）。代码块中或者成员变量中如果有类对象的话，肯定要先将类对象创建出来。所以说先执行代码块再执行构造函数，而且如果代码块中有多个成员变量，则按照成员变量的声明顺序进行构造。初始化列表的顺序, 不影响成员变量构造顺序

因为静态变量等这些在内存中属于全局变量，所以要先创建。然后类会查看成员函数的相关信息，为该类的对象中的每个成员变量分配相应的存储空间，然后再执行构造函数创建对象，如果构造函数有初始化，则将值放到准备好的内存空间中。

**构造函数执行顺序**

1. 基类构造函数。如果有多个基类，则构造函数的调用顺序是该基类在派生类中出现的顺序，而不是他们在成员初始化列表中的顺序。
2. 成员类对象构造函数。如果有多个成员类构造函数调用顺序是对象在类中被声明的顺序，而不是他们在成员初始化列表中的顺序
3. 派生类构造函数

**析构函数顺序**

1. 派生类析构函数
2. 成员类对象的析构函数
3. 调用基类的析构函数



#### C++成员变量的初始化顺序问题

```c++
class A
{
private:
    int n1;
    int n2;
public:
    A():n2(0),n1(n2+2){}
    void Print(){
        cout << "n1:" << n1 << ", n2: " << n2 <<endl;
    }
};

int main()
{
    A a;
    a.Print();
    return 1;
}

//输出：n1: 是一个随机数；n2: 0
//有的编译器是n1:2, n2=0
```

1. 成员变量在使用初始化列表初始化时，只与定义成员变量的顺序有关，与构造函数中初始化成员列表的顺序无关。因为成员变量的初始化次序是根据变量在内存中次序有关，而内存中的排列顺序早在编译期就根据变量的定义次序决定了。
2. 如果不使用初始化列表初始化，在构造函数内初始化时，此时与成员变量在构造函数中的位置有关。
3. 类成员在定义时，是不能初始化的
4. 类中const成员常量必须在构造函数初始化列表中初始化。
5. 类中static成员变量，必须在类外初始化。

**变量的初始化顺序：**

1. 初始化基类中的静态成员变量和静态代码块，按照在程序中出现的顺序初始化；
2. 初始化派生类中的静态成员变量和静态代码块，按照在程序中出现的顺序初始化；
3. 初始化基类的普通成员变量和代码块，再执行父类的构造方法；
4. 初始化派生的普通成员变量和代码块，在执行子类的构造方法；

### c++新特性

#### const与constexpr

```
c++98：const有两个语义，分别是 常量和只读
c++11之后： 
	const:表示只读
	constexpr:表示常量，在编译期会被计算出来
```

#### c++11多线程

```
创建线程：
	thread t1();

管理线程：
	void join()	等待线程结束并清理资源（会阻塞）
	bool joinable()	返回线程是否可以执行join函数
	void detach()	将线程与调用其的线程分离，彼此独立执行（此函数必须在线程创建时立即调用，且调用此函数会使其不能被join）

线程传参：
    值引用必须用std::ref(),包装
void func(int& a)
{   
    cout<<"func &"<<endl;
    a+=10;
}
int a=10;
thread t1(func,std::ref(a);
          

原子操作与锁：
    atomic_int n = 0; 对n的操作都是线程安全的

互斥锁：std::mutex
void lock():
	将mutex上锁。如果mutex已经被其它线程上锁，那么会阻塞，直到解锁；如果mutex已经被同一个线程锁住，那么会产生死锁。
void unlock():
	解锁mutex，释放其所有权。如果有线程因为调用lock()不能上锁而被阻塞，则调用此函数会将mutex的主动权随机交给其中一个线程；如果mutex不是被此线程上锁，那么会引发未定义的异常。
bool try_lock():
	尝试将mutex上锁。如果mutex未被上锁，则将其上锁并返回true；如果mutex已被锁则返回false。
	
递归锁：std::recursive_mutex，可以支持多次上锁，不会发生死锁现象


```



lock_guard与unique_lock的区别

```c++
template<typename _Mutex>
class lock_guard
{
public:
  typedef _Mutex mutex_type;

  explicit lock_guard(mutex_type& __m) : _M_device(__m)
  { _M_device.lock(); }

  lock_guard(mutex_type& __m, adopt_lock_t) noexcept : _M_device(__m)
  { } // calling thread owns mutex

  ~lock_guard()
  { _M_device.unlock(); }

  lock_guard(const lock_guard&) = delete;
  lock_guard& operator=(const lock_guard&) = delete;

private:
  mutex_type&  _M_device;
};


mutex lock_;
lock_guard<mutex> l1(lock_);
本质上就是初始化的时候上锁，lock_guard对象销毁的时候解锁
lock_guard的拷贝和复制构造函数都delete了
    
 
    
 unique_lock的使用更为灵活
 比如 unique_lock<mutex> lockguard(m1, try_to_lock_t());	//正常,对已经加锁的 mutex 尝试加锁不会导致崩溃

也可以提前解锁
void CriticalSection_1()
{
	unique_lock<mutex> lockguard(m1);
	lockguard.unlock();
	CriticalSection_2();
}
```



#### 左值与右值、左值引用与右值引用、万能引用与完美转发

左值：能够用&取地址的表达式是左值表达式。

- ​    函数名和变量名（实际上是函数指针和具名变量，具名变量等）
- ​    返回左值引用的函数调用
- ​    前置自增/自减运算符连接的表达式++i/--i
- ​    由赋值运算符或复合赋值运算符连接的表达式(a=b、a+=b、a%=b）
- ​    解引用表达式*p
- ​    字符串字面值"abc"等。

纯右值：满足下列条件之一：本身就是赤裸裸的、纯粹的字面值。或求值结果相当于字面值或是一个不具名的临时对象。

- ​    除字符串字面值以外的字面值
- ​    返回非引用类型的函数调用
- ​    后置自增/自减运算符连接的表达式i++/i--
- ​    算术表达式（a+b、a&b、a<<b）
- ​    逻辑表达式（a&&b、a||b、~a）
- ​    比较表达式（a==b、a>=b、a<b）
- ​    取地址表达式（&a）等。

将亡值：在C++11之前的右值和C++11中的纯右值是等价的。C++11中的将亡值是随着右值引用的引入而新引入的。换言之“将亡值”概念的产生，是由右值引用的产生而引起的，将亡值与右值引用息息相关。所谓的将亡值表达式，就是下列表达式：

- ​    返回右值引用的函数的调用表达式
- ​    转换为右值引用的转换函数的调用表达式（std::move）



左值引用与右值引用

```c++
int a=10;
int& b=a;

/*
  a是左值
  b是左值引用，引用的是a
  本质上是一个常量指针，这个指针值指向的地址就是，引用对象的地址，是一个常量不可更改
  
*/



int getval()
{	
   int a=10;
	return a;
}

int&& c=getval();

/*
  getval()的返回值是一个右值，且是一个将亡值
  c是右值引用，延长了其生命周期
*/
```



为什么要有右值与右值引用？为了移动语义：std::move()

```c++
/*
考虑这么一个场景，有一个表单
string 是这个表的名字，std::vector<int>储存的是表的数据
*/

std::map<string, std::vector<int>> mapTable;


void fun()
{
    std::vector<int> vecRow;
    for(){
    vecRow.push_back(...);
    }
    mapTable["列名1"] = vecRow;
}

```

​	先新建了一个表项，然后给这个循环添加元素，最后在总表单中添加刚刚写好的一个表项。

```c++
mapTable["列名1"] = vecRow;
```

​	首先执行这行代码的时候，会调用vector的拷贝构造（如果是其他自定义的类，对应调用其拷贝构造）完成在总表单的初始化，函数调用后vecRow被销毁，又会调用其析构造函数。

​	但实际上我们要在总表单上添加的数据项和这个临时变量的值都是一样的，只是因为这个临时变量在出func这个作用域后，会被销毁，所以不能直接引用。这个时候就有了移动语义了

```c++
std::vector<int> vecRow;
for(){
vecRow.push_back(...);
}
mapTable["列名1"] = std::move(vecRow);
```

std::move(vecRow);把vecRow变成一个右值，此时就会调用移动构造，移动构造只是把控制权交给别人，没有发生数据复制。可以提高效率。



为什么会有万能引用？减少函数重载的代码量



先看一个示例代码

```c++
void func(int&& a)
{
    cout<<"int&&"<<endl;
}

void func(int& a)
{
    cout<<"int&"<<endl;
}

int get_val(){
    return 10;
}

int main(){
    int a=10;
    func(a);
    func(get_val());
}

/*
输出结果
int&
int&&  
*/
```



按上面的结果，如果&与&&是有函数重载的

```c++
template<class T>
void func(T& a)
{
    
}
```







# 操作系统&&计算机体系结构

CPU的基本结构

# 数据结构与算法

### 排序

```c++
void quick_sort(int q[], int l, int r)
{
    if (l >= r) return;

    int i = l - 1, j = r + 1, x = q[l + r >> 1];
    while (i < j)
    {
        do i ++ ; while (q[i] < x);
        do j -- ; while (q[j] > x);
        if (i < j) swap(q[i], q[j]);
    }
    quick_sort(q, l, j), quick_sort(q, j + 1, r);
}

void merge_sort(int q[], int l, int r)
{
    if (l >= r) return;

    int mid = l + r >> 1;
    merge_sort(q, l, mid);
    merge_sort(q, mid + 1, r);

    int k = 0, i = l, j = mid + 1;
    while (i <= mid && j <= r)
        if (q[i] <= q[j]) tmp[k ++ ] = q[i ++ ];
        else tmp[k ++ ] = q[j ++ ];

    while (i <= mid) tmp[k ++ ] = q[i ++ ];
    while (j <= r) tmp[k ++ ] = q[j ++ ];

    for (i = l, j = 0; i <= r; i ++, j ++ ) q[i] = tmp[j];
}

```



### 链表

```
auto low=head;
auto fast=head;

while(fast!=tail&&fast->next!=tail){
    low=low->next;
    fast=fast->next->next;
}
auto mid=low;



right
auto low=head;
auto fast=head;

while(fast!=tail&&fast->next->next!=tail){
    low=low->next;
    fast=fast->next->next;
}
auto mid=low;

```



### 二叉树

### 双指针

### 堆&&栈&&队列

### 回溯

### 动态规划

一般的动态规划问题

背包类问题

```
分类解题模板

背包问题大体的解题模板是两层循环，分别遍历物品nums和背包容量target，然后写转移方程，
根据背包的分类我们确定物品和容量遍历的先后顺序，根据问题的分类我们确定状态转移方程的写法

首先是背包分类的模板：
1、0/1背包：外循环nums,内循环target,target倒序且target>=nums[i];
2、完全背包：外循环nums,内循环target,target正序且target>=nums[i];
3、组合背包：外循环target,内循环nums,target正序且target>=nums[i];
4、分组背包：这个比较特殊，需要三重循环：外循环背包bags,内部两层循环根据题目的要求转化为1,2,3三种背包类型的模板

然后是问题分类的模板：
1、最值问题: dp[i] = max/min(dp[i], dp[i-nums]+1)或dp[i] = max/min(dp[i], dp[i-num]+nums);
2、存在问题(bool)：dp[i]=dp[i]||dp[i-num];
3、组合问题：dp[i]+=dp[i-num];

（1）如果是完全背包，即数组中的元素可重复使用并且不考虑元素之间顺序，arrs 放在外循环（保证 arrs 按顺序），target在内循环。且内循环正序。
（2）如果组合问题需考虑元素之间的顺序，需将 target 放在外循环，将 arrs 放在内循环，且内循环正序


```



子数组问题

子序列问题

公共子序列问题

### 贪心

# AI算法

### 常见算子的时间复杂度分析、参数分析、计算量分析、梯度计算分析



#### 卷积

```
class torch.nn.Conv2d
(in_channels, out_channels, kernel_size, 
stride=1, padding=0, dilation=1, groups=1, bias=True, 
padding_mode='zeros', device=None, dtype=None
)
```

##### 卷积的参数量

out_channels✖in_channels✖kernel_w✖kernel_h+out_channels

##### 卷积的FLOPs（不考虑strinde的话）

out_channels✖in_channels✖kernel_w✖kernel_h✖2✖w✖h

##### 卷积的时间复杂度

out_c✖in_c✖w✖h✖kernel_size

```c++
for(int x=0;x<out_c;x++){
    for(int y=0;y<h;y++){
        for(int z=0;z<w;z++)
            
            for(int c=0;c<in_c;c++){

              for(int num=0;num<kernel_size();num++){

              }
                    
                
 
            }
            
    }
}
```



#### 激活函数

```
Relu
Silu
Gelu
sigmod

```



#### POOL

```
最大池化
平均池化
自适应池化

池化的作用：下采样，降低计算量
梯度传播是怎么传播的
```



#### DropOut

```
def dropout(x, level):
    if level < 0. or level >= 1: #level是概率值，必须在0~1之间
        raise ValueError('Dropout level must be in interval [0, 1[.')
    retain_prob = 1. - level

    # 我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样
    # 硬币 正面的概率为p，n表示每个神经元试验的次数
    # 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。
    random_tensor = np.random.binomial(n=1, p=retain_prob, size=x.shape) #即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了
    print(random_tensor)

    x *= random_tensor
    print(x)
    x /= retain_prob

    return x

#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  
x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)
dropout(x,0.4)



比如 输入的是【1，2，3，4，5，6】，然后丢弃的概率是0.5
则丢弃后，结果是 【0，2，0，0，5，6】
然后为为了保持数据的强度（或者说为了让数据的期望保持一致），或除以（1-0.5），也就是乘2，得到
0，4，0，0，10，12


梯度传播，0的梯度就是变为0，其他的梯度对应的除以/（1-0.5）

```





#### Batch Normalization

```
classtorch.nn.BatchNorm2d
(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)


对每个通道的特征做归一化
(B,N,H,W)
 
batch 1  (N,H,W)   b1_channel1(H,W)  b1_channel2(H,W)
batch 2  (N,H,W)   b2_channel1(H,W)  b2_channel2(H,W)
batch 3  (N,H,W)   b3_channel1(H,W)  b3_channel2(H,W)

把b1_channel1+b2_channel1+b3_channel1 加起来一共3HW个元素，做归一化


计算公式
(x-E(x))/(标准差)*r+b

对于训练的时候，均值和方差时通过计算出来的，并使用滑动平均的方法更新均值和方差
Et(x)=Et-1(x)*0.9+Et(x)*0.1

伽马和贝塔就是通过梯度下降学习得来

可学习参数一共num_features*2
总共的参数一共num_features*4


推理的时候 均值、标准差、伽马和贝塔，一共四个参数，均值和标准差是全局的均值和方差


(x-e)/a

```

![image-20240521183956226](/home/hp/.config/Typora/typora-user-images/image-20240521183956226.png)

进一步简化，可以把四个参数化简为两个



#### layernorm

```
对每个序列所有的特征做归一化

输入数据
class
torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, bias=True, device=None, dtype=None)

batch_size,seq_len,dims

LayerNorm((seq_len,dims))

也就是：
batch_size1 seq_len，dims  在batch_size1内的seq_len✖dims个元素里做归一化，在对seq_len✖dims个元素做伽马和贝塔的放缩
batch_size2 seq_len, dims

```



#### RMSnrom

```
和layernorm一样，对每个序列所有的特征做归一化
只是计算方式不一样，计算速度会变快


class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight

```







#### Transformer

##### 模型总体结构

![image-20240520125329189](/home/hp/.config/Typora/typora-user-images/image-20240520125329189.png)

##### 多头注意力

1. qkv通过三个线性变换成QKV
2. dropout
3. 自注意力
4. S=（Q@K）*scale
5. softmax
6. O=S@V
7. o经过线性变换输出



##### 线性变换

```python
#@save
class MultiHeadAttention(nn.Module):
    """多头注意力"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)

    def forward(self, queries, keys, values, valid_lens):
        # queries，keys，values的形状:
        # (batch_size，查询或者“键－值”对的个数，num_hiddens)
        # valid_lens　的形状:
        # (batch_size，)或(batch_size，查询的个数)
        # 经过变换后，输出的queries，keys，values　的形状:
        # (batch_size*num_heads，查询或者“键－值”对的个数，
        # num_hiddens/num_heads)
        queries = transpose_qkv(self.W_q(queries), self.num_heads)
        keys = transpose_qkv(self.W_k(keys), self.num_heads)
        values = transpose_qkv(self.W_v(values), self.num_heads)

        if valid_lens is not None:
            # 在轴0，将第一项（标量或者矢量）复制num_heads次，
            # 然后如此复制第二项，然后诸如此类。
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        # output的形状:(batch_size*num_heads，查询的个数，
        # num_hiddens/num_heads)
        output = self.attention(queries, keys, values, valid_lens)

        # output_concat的形状:(batch_size，查询的个数，num_hiddens)
        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)

#@save
def transpose_qkv(X, num_heads):
    """为了多注意力头的并行计算而变换形状"""
    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，
    # num_hiddens/num_heads)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
    # num_hiddens/num_heads)
    X = X.permute(0, 2, 1, 3)

    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
    # num_hiddens/num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])


#@save
def transpose_output(X, num_heads):
    """逆转transpose_qkv函数的操作"""
    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)

```

​	对于输入q=k=v,它们的维度（batch_size,seq_len,dims）,经过三个线性层做线性变换，需要注意的是query_size==num_hiddens==dims，即经过线性层变换后，输出张量的维度不变

```
self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
```

​	通过reshape来等价的实现多头注意力,把最后一个维度seq_len，分成（num_heads, -1），再把num_heads往前移动一个维度，

即（batch_size,num_heads,查询或者“键－值”对的个数,num_hiddens/num_heads),再把batch_size,num_heads,合并

```python
X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

# 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
# num_hiddens/num_heads)
X = X.permute(0, 2, 1, 3)

# 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
# num_hiddens/num_heads)
return X.reshape(-1, X.shape[2], X.shape[3])
```



##### 自注意力

输入QKV,其维度为（m,seq_len,dims），分别是每个（seq_len,dims）做attention，也就是独立的做m个attention,每一个attention按如下方式计算

```python
attn_weight = query @ key.transpose(-2, -1) * scale_factor
attn_weight = torch.softmax(attn_weight, dim=-1)
result = attn_weight @ value
```



##### 带MASK自注意力

因为在大模型里的里，模型的输出是自回归的，也就是V的维度会越变越大，从（seq_len,dims）到（seq_len+1,dims），对于V中的每一行数据，也就是一个token，只能与前面的token做自注意力，通过MASK可以把Q@K里面的注意力分数设置为0

```
MASK=[
0,-INF,-INF,----,-INF
0,0,-INF,-INF,---,-INF
----------------------
0,0,0,----------,0,-INF
0,0,0,----------,0,0,0,
]


attn_weight = query @ key.transpose(-2, -1) * scale_factor
attn_weight += MASK
attn_weight = torch.softmax(attn_weight, dim=-1)
result = attn_weight @ value
```

##### MLP

其实就是两个线性层，中间加一个激活函数

```python
#@save
class PositionWiseFFN(nn.Module):
    """基于位置的前馈网络"""
    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))

```

##### 参数和计算量分析

记transformer模型的层数为  l，隐藏层维度为 h ，注意力头数为  a,词表大小为  V，训练数据的批次大小为 b，序列长度为  s



权重的参数分析

```
对于一个transformer block，含有一个multi-head-attention和MLP

multi-head-attention，有4个线性变换矩阵(不带bias)，QKV的维度为（h*h）,O的维度为（h*h），参数量为4h*h
MLP,有两个线性变换矩阵（不带bias），分别是(h,4h),(4h,h),参数量为8h*h

总的参数量为12lh*h
```

### Transformer完全解析

### 多目标检测与跟踪

#### 单阶段算法：YOLOv1-YOLOv8

#### 端到端：DETR,RT-DETR

#### 检测后跟踪：ByteTrack

### 面试常见问题汇总

# CUDA编程

### CUDA编程模型

### CUDA内存结构

### CUDA性能分析（Nsight systerm与Nsight compute的使用）

### CUDA算子编写与优化

#### reduce

​	基于warp原语实现warp reduce和block reduce，并通过原子操作实现多个block之间的reduce。当输入数据量较大，优化单个线程处理的数据量，算子速度提高50%。

​	事实上block reduce更适合作为一个函数去给别的算子调用。

​	但如果单论求一行的reduce操作的话实际上的话，使用一个warp取处理一行数据就时最快的，最后调用一个warp reduce就可以了，因为这样的显存带宽利用率时最高的。

##### 基础版本

```c++
//基于warp原语，实现warp_reduce
//val 是每个线程寄存器里的值
//返回后，每个线程寄存器的值，都是32个线程val值的和
template<int WARPSIZE>
__device__ __forceinline__ float warp_reduce_add(float val)
{
    for(int mask=WARPSIZE>>1;mask>=1;mask>>=1){
        val+=__shfl_xor_sync(0xffffffff,val,mask);
    }

    return val;
}


//对block里的所有线程取和
//调用两次 warp_reduce即可
template<int WARPSIZE,int BLOCKSIZE>
__device__ float block_reduce_add(float val)
{
    int tid=threadIdx.x;
    int warp_num = (BLOCKSIZE+WARPSIZE-1)/WARPSIZE;
    int warp_index=tid/WARPSIZE;
    int warp_thread=tid&(WARPSIZE-1); //取余，把 % 变成位运算可以提高计算速度

    __shared__ float data[WARPSIZE];

    val=warp_reduce_add<WARPSIZE>(val);

    if(warp_thread==0) data[warp_index]=val;
    __syncthreads();

    val = (warp_thread<warp_num)?data[warp_thread]:0;
    __syncthreads();

    val=warp_reduce_add<WARPSIZE>(val);

    return val;
}


//
template<int WARPSIZE,int BLOCKSIZE>
__global__ void multi_block_reduce_add(float *d_in, float *d_out)
{
    int tid=threadIdx.x;
    int block_index = blockIdx.x;

    const float* data_ptr = d_in+block_index*BLOCKSIZE;
    float val = data_ptr[tid];

    val = block_reduce_add<WARPSIZE,BLOCKSIZE>(val);
    if (tid == 0) atomicAdd(d_out, val);
}
```

##### 使用warp处理一行数据，并增大warp处理的数据里量，减少atomicAdd的数量，速度可以提高50%～到100%

```c++
template<int WARPSIZE>
__device__ __forceinline__ float warp_reduce_add(float val)
{
    for(int mask=WARPSIZE>>1;mask>=1;mask>>=1){
        val+=__shfl_xor_sync(0xffffffff,val,mask);
    }

    return val;
}


//这里的BLOCKSIZE 实际上不是一个block里面的线程数，而是一个warp要处理的数据
//比如一共有N和数，512一组，一共有M=N/512组
//则kernel启动的时候，对应分配 <<<m,32>>>
//一个warp，32个线程通过循环去处理求和，在warp_reduce,求warp内的和
//需要进行的atomicAdd操作是，M次，所以增加一个warp要处理的数据量，可以减少atomicAdd操作
//但增加到一定量后，提速基本就停了，因为此时一个warp要处理的数据量大，对应的block数量少，atomicAdd相互竞争的情况也就少了，达到一个平衡
//我自己的机器3080ti 大概在一个warp处理1024个数据的时候，差不多就停止提速了
template<int WARPSIZE,int BLOCKSIZE>
__global__ void warp_level_reduce(float *d_in, float *d_out){

    int tid=threadIdx.x;
    int nums=(BLOCKSIZE+WARPSIZE-1)/WARPSIZE;
    int gtid=blockIdx.x*BLOCKSIZE+tid;    //每个block前32个线程

    float val=0;
    for(int index=0;index<nums;index++){
        val+=d_in[gtid+WARPSIZE*index];
    }
    val=warp_reduce_add<WARPSIZE>(val);
    if (tid == 0) atomicAdd(d_out, val);
}

```





#### layernorm

#### softmax

总体思路，根据数据量的大小采取不同的优化策略。

- 当数据量比较小时（PS：这里的数据量指的是，一行的数据量，比如输入数据维度是[ROWS，COLS]，数据量大指的是COLS过大）。此时采用一个warp处理一行数据，然后声明了一个寄存器数组去缓存每一行的数据，这样可以避免反复从global memory中读取。
- 当数据量比较大时（我自己的电脑的话，COLS>2048差不多就溢出了，一溢出的话速度直线下降，电脑直接卡死）。此时采用一个block处理一行数据，然后用shared来缓存每一行的数据。

```c++
template<int WARPSIZE>
__device__ __forceinline__ float warp_reduce_sum(float val)
{
    for(int mask=WARPSIZE>>1;mask>=1;mask>>=1){
        val += __shfl_xor_sync(0xffffffff,val,mask);
    }

    return val;

}

template<int WARPSIZE>
__device__ __forceinline__ float warp_reduce_max(float val)
{
    for(int mask=WARPSIZE>>1;mask>=1;mask>>=1){
        val =fmax(val,__shfl_xor_sync(0xffffffff,val,mask));
    }

    return val;
}

template<int WARPSIZE,int BLOCKSIZE>
__device__ float block_reduce_sum(float val)
{
    int tid=threadIdx.x;
    int warp_num = (BLOCKSIZE+WARPSIZE-1)/WARPSIZE;
    int warp_index=tid/WARPSIZE;
    int warp_thread=tid&(WARPSIZE-1); //取余

    __shared__ float data[WARPSIZE];

    val=warp_reduce_sum<WARPSIZE>(val);

    if(warp_thread==0) data[warp_index]=val;
    __syncthreads();

    val = (warp_thread<warp_num)?data[warp_thread]:0.0f;
    __syncthreads();

    val=warp_reduce_sum<WARPSIZE>(val);

    return val;
}

template<int WARPSIZE,int BLOCKSIZE>
__device__ float block_reduce_max(float val)
{
    int tid=threadIdx.x;
    int warp_num = (BLOCKSIZE+WARPSIZE-1)/WARPSIZE;
    int warp_index=tid/WARPSIZE;
    int warp_thread=tid&(WARPSIZE-1); //取余

    __shared__ float data[WARPSIZE];

    val=warp_reduce_max<WARPSIZE>(val);

    if(warp_thread==0) data[warp_index]=val;
    __syncthreads();

    val = (warp_thread<warp_num)?data[warp_thread]:-1000.0f;
    __syncthreads();

    val=warp_reduce_max<WARPSIZE>(val);

    return val;
}


//<<<rows,32>>>
template<int WARPSIZE,int BLOCKSIZE,int COLS_>
__global__ void warp_softmax(float* in_data,float* out_data,int rows,int cols)
{
    float rdata[COLS_/WARPSIZE];
    int thread_per_nums=COLS_/WARPSIZE;

    int tid=threadIdx.x;
    int block_index = blockIdx.x;

    const float* data_ptr=in_data+block_index*cols;
    float* save_ptr=out_data+block_index*cols;

    float max_val=-1000.0f;
    
    for(int i=0;i<thread_per_nums;i++){
        rdata[i]=data_ptr[tid+i*WARPSIZE];
        max_val=fmax(max_val,rdata[i]);
    }

    max_val=warp_reduce_max<WARPSIZE>(max_val);

    float sum_val=0.0f;
    for(int i=0;i<thread_per_nums;i++){
        rdata[i]=exp(rdata[i]-max_val);
        sum_val+=rdata[i];
    }

    sum_val=warp_reduce_sum<WARPSIZE>(sum_val);

    for(int i=0;i<thread_per_nums;i++){
        save_ptr[tid+i*WARPSIZE]=rdata[i]/sum_val;
    }

}


//256/32 8
//<<<rows/8,256>>>
template<int WARPSIZE,int BLOCKSIZE,int COLS_>
__global__ void block_warp_softmax(float* in_data,float* out_data,int rows,int cols)
{
    float rdata[COLS_/WARPSIZE];
    int thread_per_nums=COLS_/WARPSIZE;

    int tid=threadIdx.x;
    int warp_num=BLOCKSIZE/WARPSIZE;
    int warp_index=tid/WARPSIZE;
    int warp_thread=tid&(WARPSIZE-1);

    int block_index = blockIdx.x;

    const float* data_ptr=in_data+block_index*cols*warp_num+warp_index*cols;
    float* save_ptr=out_data+block_index*cols*warp_num+warp_index*cols;

    float max_val=-1000.0f;
    
    for(int i=0;i<thread_per_nums;i++){
        rdata[i]=data_ptr[warp_thread+i*WARPSIZE];
        max_val=fmax(max_val,rdata[i]);
    }

    max_val=warp_reduce_max<WARPSIZE>(max_val);

    float sum_val=0.0f;
    for(int i=0;i<thread_per_nums;i++){
        rdata[i]=exp(rdata[i]-max_val);
        sum_val+=rdata[i];
    }

    sum_val=warp_reduce_sum<WARPSIZE>(sum_val);

    for(int i=0;i<thread_per_nums;i++){
        save_ptr[warp_thread+i*WARPSIZE]=rdata[i]/sum_val;
    }

}


//<<<rows,256>>>
template<int WARPSIZE,int BLOCKSIZE,int COLS_>
__global__ void block_warp_shared_softmax(float* in_data,float* out_data,int rows,int cols)
{
    __shared__ float sdata[COLS_];
    int block_per_num = COLS_/BLOCKSIZE;
    int tid=threadIdx.x;
    int block_index=blockIdx.x;

    const float* data=in_data+block_index*cols;
    float* out=out_data+block_index*cols;

    float max_val=0.0f;
    for(int i=tid;i<cols;i+=BLOCKSIZE){
        sdata[i]=data[i];
        max_val=fmax(max_val,sdata[i]);
    }

    max_val = block_reduce_max<WARPSIZE,BLOCKSIZE>(max_val);

    float sum_val=0.0f;
    for(int i=tid;i<cols;i+=BLOCKSIZE){
        sdata[i] = exp(sdata[i]-max_val);
        sum_val+=sdata[i];
    }

    sum_val= block_reduce_sum<WARPSIZE,BLOCKSIZE>(sum_val);

    for(int i=tid;i<cols;i+=BLOCKSIZE){
        out[i]=sdata[i]/sum_val;
    }
}
```



#### 激活函数类

#### gemv

#### flash attention

### CUDA的Stream

### CUDA编程总结

### CUDA面试常见问题汇总

```
1. ### CUDA的线程组织结构

2. ### CUDA的存储体系结构，每一种存储的优缺点，该如何合理使用。

```
   全局内存：
   这是GPU中最大的内存，即我们常说的HBM内存，可以被所有块上的所有线程访问，当我们在GPU中初始化一个值而不指定其存储位置时，它会自动存储在全局内存中。然而，访问全局内存通常比其他内存类型慢，因此需要进行优化以避免性能下降，可以通过合并内存访问和使用共享内存来优化性能。

   共享内存：
   同一个Block内的线程可以通过共享内存共享数据。相比访问全局内存至少快个10倍，但共享内存的容量有限（通常为几十KB），无法被其他线程块访问。由于共享内存和缓存内存提供快速的访问速度，因此我们经常在计算过程中使用它们来存储数据。典型的方法是首先将所有数据从 CPU 复制到 GPU 并将其存储在全局内存中。然后，我们将数据分解成更小的部分（块）并将它们推送到共享内存中进行计算。计算完成后，结果将被推回全局内存。

   本地内存（Local Memory）
   每个线程都可以使用自己的本地内存，可以在其中存储临时变量。它具有最小的范围并且专用于每个单独的线程。


   GPU由多个SM组成，每个SM内有多个warp调度器。

   全局内存
   L2 cache
   L1 cache

   共享内存->物理上和L1 cache是在一起的，可以配置大小。一个block里共享，使用时需要注意同步。需要注意bank conflict
   寄存器->线程私有，自动分配
   local内存->寄存器溢出的时候会使用

   ```

   

3. ### GPU每一代的新特性有了解过吗？应该从哪里去了解详细信息？

   ```
   nvida官网，有一些架构的白皮书。
   CUDA tooklits里有对应架构的调优指南，这里讲得具体一些。

   https://www.nvidia.cn/technologies/
   Pascal架构    1050ti
   Volta架构
   Turing架构
   Ampere架构   RTX3080TI


   举例，比如在Ampere架构架构上提供了原生的Reduce操作，支持FP64，BFloat16的tensorcore


   ```

   

4. ### CUDA stream的概念，为什么要使用多个stream？

   ```
   CUDA Stream是指一堆异步的CUDA操作，它们按照主机（Host）代码确定的顺序在设备（Device）上执行。这些操作可以包括主机和设备之间的数据传输、内核（Kernel）启动以及其他由主机发起但由设备处理的命令。Stream允许操作在其中排队，并确保它们在先前的所有操作之后执行，从而提供了一种管理和组织异步操作的方式。

   当GPU在执行一个内核（Kernel）时，主机（Host）可能需要将数据从内存传输到GPU或从GPU传输回内存。通过使用多个Stream，可以在一个Stream中执行内核操作，同时在另一个Stream中执行数据传输操作。这样，数据传输和内核执行可以重叠进行，从而提高GPU的利用率。
   ```

   

5. ### GPU和CPU分别适合执行哪些程序？结合它们的硬件架构解释一下为什么它们有各自的优势。

6. ### 说明一下神经网络加速器与CPU、GPU的区别，他们各自有何优势？

7. ### 半精度浮点数FP16各个部分的具体位数，为什么要有半精度浮点数？

8. ### TensorCore的加速原理

9. ### MPI，OpenMP以及CUDA各自适用的加速场景。

10. ### RDMA相关问题。

11. ### 平时如何进行kernel的优化，会用到哪些工具？

12. ### CPU上哪些并行优化方法？

13. ### ARM相关的库有了解过吗？

14. ### PTX有了解过吗？

15. ### roofline模型有什么用？如何确定最优的BLOCK_SIZE。

16. ### GPU资源调度有哪些方法？

17. ### 稀疏矩阵的存储格式有哪些？稀疏矩阵的应用场景？稀疏矩阵计算与稠密矩阵计算有何不同？

18. ### 如何计算CPU指令的吞吐量和时延?
   ```



# 推理框架

### 带显卡&&服务器端&&桌面设备：TensorRT

### 移动端&&嵌入式设备：ncnn

### 大模型推理：FastLLM、VLLM

### 推理框架总结&&常见问题汇总

# 模型量化

### 量化基本知识

### 量化加速原理

### 量化常见问题总结

# 大模型推理加速

### flash attention

### page attention

### kv cache

### 大模型量化&&前沿的量化论文

# 手撕代码

### leetcode

### c++的一些设计模式

#### 生产者消费者模型

```c++

#include<algorithm>
#include<thread>
#include<mutex>
#include<condition_variable>
#include<queue>
#include<iostream>
#include<chrono>

using namespace std;

static int index = 0;
class Prodeuce_consumer {
	
public:	
	Prodeuce_consumer() {
		max_size = 10;
	}

	Prodeuce_consumer(int size) {
		max_size = size;
	}

	Prodeuce_consumer(const Prodeuce_consumer& p1) {
		max_size = p1.max_size;
	}

	Prodeuce_consumer& operator =(const Prodeuce_consumer& p1) {
		if (&p1 == this) return *this;
		Prodeuce_consumer p = Prodeuce_consumer(p1.max_size);
		return p;
	}

	virtual ~Prodeuce_consumer() {

	}

	void run() {
		thread t1(&Prodeuce_consumer::procuder,this);
		thread t2(&Prodeuce_consumer::consumer, this);
		thread t3(&Prodeuce_consumer::consumer, this);
		thread t4(&Prodeuce_consumer::consumer, this);

		t1.join();
		t2.join();
		t3.join();
		t4.join();
	}


	void procuder() {
			
		while (1) {
			
			this_thread::sleep_for(chrono::milliseconds(100));
			{
				unique_lock<mutex> ul(mu);
				cond.wait(ul, [&]() {
					return que.size() < max_size;
					});
				que.push(index++);
				cout << "producer is " << index << "  ";
				cout << "que size is " << que.size() << endl;
			}
			


		}

	}


	void consumer() {

			
		while (1) {
 
			{
				lock_guard<mutex> l(mu);
				if (!que.empty()) {
					int cur = que.front();
					que.pop();
					cout <<"ID IS   "<<this_thread::get_id()<< "   consumer run " << cur << endl;
					cond.notify_all();
				}
			}
			this_thread::sleep_for(chrono::milliseconds(500));
			
		}


	}

private:
	int max_size;
	queue<int> que;
	mutex mu;
	condition_variable cond;

};




int main()
{
	Prodeuce_consumer p1;
	p1.run();
}

```

#### 线程安全的单例模式

```c++
class Test {

	Test& get_instance() {
		static Test t1;
		return t1;
	}

private:
	Test() {};
	Test(const Test& t) = delete;
	Test& operator=(const Test& t) = delete;
};


template <class T>
class Singleton
{
private:
    Singleton() = default;
    ~Singleton() = default;
    Singleton(const Singleton&)=delete;
    Singleton& operator=(const Singleton&)=delete;
 
public:
    static T* instance()
    {
        std::call_once(_flag, [&](){
            _instance = new T();
        });
        return _instance;
    }
 
private:
    static T* _instance;
    static std::once_flag _flag;
};
 

```



#### 智能指针

```c++



//share_ptr
template<class T>
class SmartPtr{
public:
	SmartPtr(T* ptr = NULL): _ptr(ptr), _pcount(new int(1)) {} //构造函数
	SmartPtr(const SmartPtr& s): _ptr(s.ptr), _pcount(s._pcount){  //拷贝构造函数
		*(_pcount)++; //用指针进行count操作能够方便控制不同智能指针对象的计数值
	}
	SmartPtr<T>& operator=(const SmartPtr& s){  //重载 = ，赋值函数
		if(this != &s){ //检测自我赋值
			if(--(*(this->_pcount)) == 0){
				delete this->_ptr;
				delete this->_pcount;
			}
			_ptr = s._ptr;
			_pcount = s._pcount;
			*(_pcount)++;
		}
		return *this;
	}
	T& operator*(){
		return *(this->_ptr);
	}
	T* operator->(){
		return this->_ptr;
	}
	~SmartPtr(){ //析构函数要进行判定，count为0才会delete，delete之后避免野指针要赋值nullptr
		--(*(this->_pcount));
		if(this->_pcount == 0){
			delete _ptr;
			_ptr = nullptr;
			delete _pcount;
			_pcount = nullptr;
		}
	}
private:
	T* _ptr;
	int* _pcount;
};

template <typename T> 
class unique_ptr {
private:
  T *ptr_resource = nullptr;

public:
  // explicit构造函数是用来防止隐式转换, 即不允许写成unique_ptr<T> tempPtr = T;
  // std::move是将对象的状态或者所有权从一个对象转移到另一个对象，只是转移，没有内存的搬迁或者内存拷贝所以可以提高利用效率,改善性能.
  // move之后，raw_resource内部的资源将不能再被raw_resource使用
  explicit unique_ptr(T *raw_resource) noexcept
      : ptr_resource(std::move(raw_resource)) {}
  unique_ptr(std::nullptr_t) : ptr_resource(nullptr) {}

  unique_ptr() noexcept : ptr_resource(nullptr) {}

  // 析构时, 释放托管的对象资源
  ~unique_ptr() noexcept { delete ptr_resource; }

    
  unique_ptr(const unique_ptr<T> &) noexcept = delete;
  unique_ptr &operator=(const unique_ptr &) noexcept = delete;

public:
  //&& 是右值引用，见https://zhuanlan.zhihu.com/p/107445960
  // 允许移动语义。虽然无法复制unique_ptr，但可以安全地移动。
  // 例子：unique_ptr<Test> tPtr3(std::move(tPtr1));
  unique_ptr(unique_ptr &&move) noexcept {
    std::cout << "construct for unique_ptr&&" << std::endl;
    move.swap(*this);
  }
  // ptr = std::move(resource)
  unique_ptr &operator=(unique_ptr &&move) noexcept {
    std::cout << "operator= for unique_ptr&&" << std::endl;
    move.swap(*this);
    return *this;
  }


public:
  // overloaded operators
  T *operator->() const noexcept { return this->ptr_resource; }
  T &operator*() const noexcept { return *this->ptr_resource; }
  // 额外说明noexcept
  // noexcept C++11关键字,
  // 告诉编译器，函数中不会发生异常,有利于编译器对程序做更多的优化
  // C++中的异常处理是在运行时而不是编译时检测的。为了实现运行时检测，编译器创建额外的代码，然而这会妨碍程序优化
};

```





### cuda

#### reduce

```c++
//分配一个warp处理一个block里的数据
template<int WARPSIZE>
__device__ __forceinline__ float warp_reduce_add(float val)
{
    for(int mask=WARPSIZE>>1;mask>=1;mask>>=1){
        val+=__shfl_xor_sync(0xffffffff,val,mask);
    }
}


template<int warpsize>
__device__ warp_reduce_add(float val)
{
    for(int mask=warpsize>>1;mask>=1;mask>>=1){
        val+=__shfl_xor_sync(0xffffffff,val,mask);
    }
    return val;
}

template<int warpsize>
__device__ warp_reduce_max(float val)
{
    for(int mask=warpsize>>1;mask>=1;mask>>=1){
        val=fmax(__shfl_xor_sync(0xffffffff,val,mask),val);
    }
    return val;
}


template<int warpsize,int blocksize>
__device__ block_reduce_sum(float val)
{

    __shared__ float sdata[blocksize/warpsize];

    int tid=threadIdx.x;
    int warp_index=tid/warpsize;
    int warp_thread=tid&(warpsize-1);
    int warp_num=blocksize/warpsize;

    val=warp_reduce_add<warpsize>(val);

    if(warp_thread==0) sdata[warp_index]=val;
    __sync_threads();

    val = (warp_thread < warp_num)?sdata[warp_thread]:0;

    val=warp_reduce_add<warpsize>(val);

    return val;

}

template<int warpsize,int blocksize>
__device__ block_reduce_max(float val)
{

    __shared__ float sdata[blocksize/warpsize];

    int tid=threadIdx.x;
    int warp_index=tid/warpsize;
    int warp_thread=tid&(warpsize-1);
    int warp_num=blocksize/warpsize;

    val=warp_reduce_max<warpsize>(val);

    if(warp_thread==0) sdata[warp_index]=val;
    __sync_threads();

    val = (warp_thread < warp_num)?sdata[warp_thread]:-1000.0;

    val=warp_reduce_max<warpsize>(val);

    return val;

}
    return val;
}
```

softmax

```c++
template<int WARPSIZE,int BLCOKSIZE,int COLS>
__device__ void warp_softmax(float input,float output,int rows,int cols)
{

    constexpr int nums=COLS/WARPSIZE;
    __shared__ float sdata[nums];

    int tid=threadIdx.x;
    int row_index = blockIdx.x;

    float* row_ptr = input+row_index*cols;
    float* out_prt = output+row_index*cols;

    float val_max=-10000f;
    for(int i=0;i<num;i++){
        sdata[i]=input[tid+i*WARPSIZE];
        val_max=fmax(sdata[i],val_max);
    }

    val_max = warp_reduce_max<WARPSIZE>(val_max);

    float val_sum=0.0f;
    for(int i=0;i<num;i++){
        sdata[i]=exp(sdata[i]-val_max)
        val_sum=val_sum+sdata[i];
    }
    val_sum = warp_reduce_sum<WARPSIZE>(val_sum);


    for(int i=0;i<num;i++){
        out_prt[tid+i*WARPSIZE]]=data[i]/val_sum;
    }

}
```





### 手撕一些算子（用c++或者pytorch）

# 面试经验

